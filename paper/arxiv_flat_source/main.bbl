\begin{thebibliography}{10}

\bibitem{field1987relations}
David~J. Field.
\newblock Relations between the statistics of natural images and the response
  properties of cortical cells.
\newblock {\em Journal of the Optical Society of America A}, 4(12):2379--2394,
  1987.

\bibitem{gaier2019weight}
Adam Gaier and David Ha.
\newblock Weight agnostic neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.
\newblock arXiv:1906.04358.

\bibitem{ha2016generating}
David Ha.
\newblock Generating large images from latent vectors.
\newblock {\em Otoro Blog}, 2016.

\bibitem{johnson1984extensions}
William~B Johnson and Joram Lindenstrauss.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock {\em Contemporary Mathematics}, 26:189--206, 1984.

\bibitem{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.
\newblock arXiv:1711.00165.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{mele2025density}
M~Mele, R~Menichetti, A~Ingrosso, and R~Potestio.
\newblock Density of states in neural networks: an in-depth exploration of
  learning in parameter space.
\newblock {\em Transactions on Machine Learning Research}, 2025.
\newblock arXiv:2409.18683.

\bibitem{mellor2021neural}
Joseph Mellor, Jack Turner, Amos Storkey, and Elliot~J Crowley.
\newblock Neural architecture search without training.
\newblock In {\em International Conference on Machine Learning}, volume 139,
  pages 7588--7598. PMLR, 2021.
\newblock arXiv:2006.04647.

\bibitem{murray2010elliptical}
Iain Murray, Ryan Adams, and David MacKay.
\newblock Elliptical slice sampling.
\newblock {\em Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 541--548, 2010.

\bibitem{oquab2023dinov2}
Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Huy Vo, Marc
  Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa,
  Alaaeldin El-Nouby, et~al.
\newblock {DINOv2}: Learning robust visual features without supervision.
\newblock {\em Transactions on Machine Learning Research}, 2024.

\bibitem{secretan2008picbreeder}
Jimmy Secretan, Nicholas Beato, David~B D'Ambrosio, Adelein Rodriguez, Adam
  Campbell, and Kenneth~O Stanley.
\newblock Picbreeder: Evolving pictures collaboratively online.
\newblock In {\em Proceedings of the SIGCHI conference on Human factors in
  computing systems}, pages 1759--1768, 2008.

\bibitem{sitzmann2020siren}
Vincent Sitzmann, Julien N~P Martel, Alexander~W Bergman, David~B Lindell, and
  Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.
\newblock arXiv:2006.09661.

\bibitem{skilling2006nested}
John Skilling.
\newblock Nested sampling for general bayesian computation.
\newblock {\em Bayesian Analysis}, 1(4):833--860, 2006.

\bibitem{stanley2007cppn}
Kenneth~O Stanley.
\newblock Compositional pattern producing networks: A novel abstraction of
  development.
\newblock {\em Genetic Programming and Evolvable Machines}, 8(2):131--162,
  2007.

\bibitem{steiner2022how}
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
  Uszkoreit, and Lucas Beyer.
\newblock How to train your {ViT}? {D}ata, augmentation, and regularization in
  vision transformers.
\newblock {\em Transactions on Machine Learning Research}, 2022.
\newblock arXiv:2106.10270.

\bibitem{tancik2020fourier}
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin
  Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan~T Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7537--7547, 2020.

\bibitem{ulyanov2018deep}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Deep image prior.
\newblock {\em International Journal of Computer Vision}, 128(7):1867--1888,
  2020.
\newblock arXiv:1711.10925.

\bibitem{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.
\newblock arXiv:1904.00687.

\bibitem{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 586--595, 2018.

\end{thebibliography}
